##TODO
#
[DATASET]
train_path = /media/vinicius/dados/datasets/volta_da_ufes_20191003_augmented/data/
train_list = /media/vinicius/dados/datasets/volta_da_ufes_20191003_augmented/train.txt
target_path = /media/vinicius/dados/datasets/volta_da_ufes_20191003_augmented/labels/
test_path = /media/vinicius/dados/datasets/volta_da_ufes_20191003_augmented/data/
test_list = /media/vinicius/dados/datasets/volta_da_ufes_20191003_augmented/validation.txt
channels = 5
img_width = 600
img_height = 600
shuffle_data = on

#Para subtrair a media e dividir pelo desvio padrao
dataset_mean = 
dataset_std =

[DNN]
use_cuda = 1
device = 0
classes = 3
#use_trained_model = 
use_trained_model = 
#batch will load for each label 5 inputs images, total batch = batch_size * 5
batch_size = 1
epochs = 60

learning_rate = 0.001

#Usando ADAM
#momentum = 0.9

step_size = 20
decay_rate = 2
dropout_prob = 0.25

#torch.manual_seed() reprodutibilidade
manual_seed = 1
#each X epochs save model       
interval_save_model = 10
save_models = /media/vinicius/dados/datasets/volta_da_ufes_20191003_augmented/models/
save_log_files = /media/vinicius/dados/datasets/volta_da_ufes_20191003_augmented/results/

#Os parametros abaixo nao foram usados, o dataset jah vem entre 0-1 e depois Ã© aplicado a normalizacao (variavel TRANSFORMS)
#max_velodyne_hight 
max_normalize_mean = 1.852193

#max_velodyne_hight 
max_normalize_max = 1.852193

#max_value_above_gound
max_normalize_min = -1
#This value is computed 
max_normalize_std = 15
max_normalize_numb = 64
